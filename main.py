import os
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

PERSIST_DIR = "chroma_db"
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"


def main():
    # Sá»­ dá»¥ng qwen2.5:7b
    llm = OllamaLLM(model="qwen2.5:7b", temperature=0)

    # 1. Kiá»ƒm tra vector DB Ä‘Ã£ Ä‘Æ°á»£c build chÆ°a
    if not os.path.exists(PERSIST_DIR):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c vector DB '{PERSIST_DIR}'. HÃ£y cháº¡y build_data.py trÆ°á»›c.")
        return

    # 2. Khá»Ÿi táº¡o embeddings vÃ  náº¡p Chroma tá»« disk
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)

    # 3. PROMPT CHO QWEN2.5 (Sá»­ dá»¥ng format chuáº©n, khÃ´ng dÃ¹ng special tokens)
    template = """Báº¡n lÃ  trá»£ lÃ½ áº£o thÃ´ng minh cá»§a TrÆ°á»ng Äáº¡i há»c SÃ i GÃ²n.
    HÃ£y sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn.

    TÃ€I LIá»†U:
    {context}

    CÃ‚U Há»I: {question}

    YÃŠU Cáº¦U:
    - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c.
    - Chá»‰ dá»±a vÃ o thÃ´ng tin trong TÃ€I LIá»†U Ä‘á»ƒ tráº£ lá»i.
    - Náº¿u tÃ i liá»‡u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y tráº£ lá»i: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong Sá»• tay sinh viÃªn."
    - Tuyá»‡t Ä‘á»‘i khÃ´ng tá»± bá»‹a Ä‘áº·t thÃ´ng tin.

    TRáº¢ Lá»œI:"""

    prompt = ChatPromptTemplate.from_template(template)
    
    # 4. TÄƒng sá»‘ lÆ°á»£ng context láº¥y ra vá»›i MMR Ä‘á»ƒ Ä‘a dáº¡ng hÃ³a vÃ  tÃ¬m rá»™ng hÆ¡n
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 6}
    )

    qa_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\nğŸ¤– Chatbot SGU (Qwen2.5) Ä‘Ã£ sáºµn sÃ ng!")
    
    while True:
        query = input("\nğŸ™‹ Báº¡n: ").strip()
        if query.lower() in ['exit', 'quit', 'thoÃ¡t']: break
        
        try:
            # Láº¥y thÃ´ng tin trang Ä‘á»ƒ kiá»ƒm tra
            context_docs = retriever.invoke(query)
            pages = set([str(d.metadata.get('page') + 1) for d in context_docs]) # +1 vÃ¬ page báº¯t Ä‘áº§u tá»« 0
            print(f"ğŸ” Äang tÃ¬m á»Ÿ trang: {', '.join(pages)}...")

            response = qa_chain.invoke(query)
            print(f"ğŸ¤– AI: {response}")
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()