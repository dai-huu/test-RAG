import os
import re
import pdfplumber
from tqdm import tqdm
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

PDF_PATH = "STSV2022Phan1.pdf"
EXTRACTED_MD = "extracted_content.md"
PERSIST_DIR = "chroma_db"
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
USE_MARKDOWN = True  # True: Ä‘á»c tá»« markdown, False: Ä‘á»c trá»±c tiáº¿p PDF


def load_from_markdown(md_path):
    """Äá»c tá»« file markdown Ä‘Ã£ chá»‰nh sá»­a"""
    documents = []
    
    with open(md_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    pages = re.split(r'\n## ğŸ“„ Trang (\d+)\n', content)
    
    for i in range(1, len(pages), 2):
        if i + 1 < len(pages):
            page_num = int(pages[i])
            page_content = pages[i + 1].strip()
            
            if page_content:
                has_table = "ğŸ“Š Báº£ng" in page_content or re.search(r'\|.*\|', page_content)
                
                documents.append(Document(
                    page_content=page_content,
                    metadata={
                        "source": md_path,
                        "page": page_num,
                        "has_table": has_table,
                        "content_type": detect_content_type(page_content),
                        "char_count": len(page_content)
                    }
                ))
    
    return documents


def extract_pdf_with_tables(pdf_path):
    """TrÃ­ch xuáº¥t PDF vá»›i xá»­ lÃ½ báº£ng"""
    documents = []
    
    with pdfplumber.open(pdf_path) as pdf:
        print(f"ğŸ“„ Tá»•ng sá»‘ trang: {len(pdf.pages)}")
        
        for page_num, page in enumerate(tqdm(pdf.pages, desc="Äá»c PDF")):
            tables = page.extract_tables()
            table_texts = []
            
            for idx, table in enumerate(tables):
                if table and len(table) > 0:
                    table_str = f"\n\n[Báº¢NG {idx + 1}]\n"
                    
                    if table[0]:
                        header = [str(cell or "").strip() for cell in table[0]]
                        table_str += "| " + " | ".join(header) + " |\n"
                        table_str += "| " + " | ".join(["---"] * len(header)) + " |\n"
                        
                        for row in table[1:]:
                            if row:
                                cells = [str(cell or "").strip() for cell in row]
                                table_str += "| " + " | ".join(cells) + " |\n"
                    
                    table_texts.append(table_str)
            
            text = page.extract_text() or ""
            
            if table_texts:
                page_text = text + "\n" + "\n".join(table_texts)
            else:
                page_text = text
            
            page_text = clean_text_preserve_structure(page_text)
            
            if page_text.strip():
                documents.append(Document(
                    page_content=page_text,
                    metadata={
                        "source": PDF_PATH,
                        "page": page_num + 1,
                        "has_table": len(tables) > 0,
                        "table_count": len(tables),
                        "content_type": detect_content_type(page_text),
                        "char_count": len(page_text)
                    }
                ))
    
    return documents


def detect_content_type(text):
    """PhÃ¡t hiá»‡n loáº¡i ná»™i dung"""
    if "[Báº¢NG" in text or "ğŸ“Š Báº£ng" in text:
        return "table"
    elif re.search(r'(Äiá»u|ÄIá»€U)\s+\d+', text):
        return "regulation"
    elif re.search(r'\d+\.\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´]', text):
        return "numbered_list"
    elif re.search(r'[-â€¢]\s+', text):
        return "bullet_list"
    else:
        return "text"


def clean_text_preserve_structure(text):
    """LÃ m sáº¡ch text giá»¯ cáº¥u trÃºc"""
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
    
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.match(r'^\s*(Äiá»u|ÄIá»€U|Khoáº£n|Má»¥c|\d+\.|\[Báº¢NG)', line):
            cleaned_lines.append(line.strip())
        else:
            cleaned_lines.append(re.sub(r'\s+', ' ', line.strip()))
    
    text = '\n'.join(cleaned_lines)
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\{\}\"\'\/\%\n\|Â°Â§]', '', text, flags=re.UNICODE)
    
    return text.strip()


def validate_chunks(splits):
    """Kiá»ƒm tra chunks"""
    issues = []
    for i, split in enumerate(splits):
        content = split.page_content
        
        if len(content) < 50:
            issues.append(f"âš ï¸  Chunk {i}: QuÃ¡ ngáº¯n ({len(content)} kÃ½ tá»±)")
        
        weird_ratio = len(re.findall(r'[^\w\s\.\,\-]', content)) / (len(content) + 1)
        if weird_ratio > 0.3:
            issues.append(f"âš ï¸  Chunk {i}: Nhiá»u kÃ½ tá»± láº¡ ({weird_ratio:.1%})")
    
    if issues:
        print("\nğŸ“Š PhÃ¡t hiá»‡n má»™t sá»‘ váº¥n Ä‘á»:")
        for issue in issues[:5]:
            print(issue)
        if len(issues) > 5:
            print(f"   ... vÃ  {len(issues) - 5} váº¥n Ä‘á» khÃ¡c")


def build_vectorstore():
    print("ğŸ“š Báº¯t Ä‘áº§u xÃ¢y dá»±ng vector database...\n")
    
    # 1. Äá»c dá»¯ liá»‡u
    if USE_MARKDOWN:
        if not os.path.exists(EXTRACTED_MD):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y {EXTRACTED_MD}")
            print(f"ğŸ’¡ Cháº¡y 'python extract_pdf.py' trÆ°á»›c")
            return
        
        print(f"ğŸ“– Äá»c tá»« markdown: {EXTRACTED_MD}")
        docs = load_from_markdown(EXTRACTED_MD)
    else:
        if not os.path.exists(PDF_PATH):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y {PDF_PATH}")
            return
        
        print(f"ğŸ“– Äá»c tá»« PDF: {PDF_PATH}")
        docs = extract_pdf_with_tables(PDF_PATH)
    
    print(f"\nâœ… Äá»c Ä‘Æ°á»£c {len(docs)} trang")
    
    table_pages = sum(1 for d in docs if d.metadata.get("has_table", False))
    print(f"   ğŸ“Š Sá»‘ trang cÃ³ báº£ng: {table_pages}")
    print(f"   ğŸ“ Tá»•ng kÃ½ tá»±: {sum(d.metadata['char_count'] for d in docs):,}")

    # 2. Embedding
    print("\nğŸ§  Äang khá»Ÿi táº¡o embedding...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)

    # 3. Chunking
    print("âœ‚ï¸  Äang chia nhá» vÄƒn báº£n...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,  # TÄƒng tá»« 1500 lÃªn 2000
        chunk_overlap=300,  # TÄƒng tá»« 200 lÃªn 300
        separators=[
            "\n## ğŸ“„ Trang",  # KhÃ´ng tÃ¡ch trang
            "\n### ğŸ“Š Báº£ng",  # Giá»¯ báº£ng nguyÃªn
            "\n#### Äiá»u ",   # Giá»¯ Ä‘iá»u khoáº£n nguyÃªn
            "\n\n",           # Paragraph break
            "\n",             # Line break
            ". ",             # Sentence
            " "               # Word
        ],
        length_function=len,
    )
    splits = splitter.split_documents(docs)
    print(f"âœ… Táº¡o Ä‘Æ°á»£c {len(splits)} chunks")

    # 4. Merge chunks quÃ¡ ngáº¯n
    MIN_CHUNK_SIZE = 100
    merged_splits = []
    i = 0
    
    while i < len(splits):
        current = splits[i]
        
        # Náº¿u chunk hiá»‡n táº¡i quÃ¡ ngáº¯n, merge vá»›i chunk sau
        if len(current.page_content) < MIN_CHUNK_SIZE and i + 1 < len(splits):
            next_chunk = splits[i + 1]
            
            # Chá»‰ merge náº¿u cÃ¹ng trang hoáº·c trang liá»n ká»
            if abs(current.metadata.get('page', 0) - next_chunk.metadata.get('page', 0)) <= 1:
                merged_content = current.page_content + "\n" + next_chunk.page_content
                merged_chunk = Document(
                    page_content=merged_content,
                    metadata={
                        **current.metadata,
                        'merged': True,
                        'char_count': len(merged_content)
                    }
                )
                merged_splits.append(merged_chunk)
                i += 2  # Skip cáº£ 2 chunks
                continue
        
        merged_splits.append(current)
        i += 1
    
    print(f"ğŸ”— Merge thÃ nh {len(merged_splits)} chunks (tá»« {len(splits)})")
    splits = merged_splits

    # 5. Metadata
    for i, split in enumerate(splits):
        split.metadata["chunk_id"] = i
    
    validate_chunks(splits)

    # 6. LÆ°u Chroma
    print("\nğŸ’¾ Äang lÆ°u vector database...")
    if os.path.exists(PERSIST_DIR):
        import shutil
        shutil.rmtree(PERSIST_DIR)
    
    BATCH_SIZE = 50
    vectorstore = None
    
    for i in tqdm(range(0, len(splits), BATCH_SIZE), desc="LÆ°u chunks"):
        batch = splits[i:i + BATCH_SIZE]
        if vectorstore is None:
            vectorstore = Chroma.from_documents(batch, embeddings, persist_directory=PERSIST_DIR)
        else:
            vectorstore.add_documents(batch)
    
    print(f"\nâœ… HoÃ n táº¥t! LÆ°u {len(splits)} chunks táº¡i: {PERSIST_DIR}")
    print(f"ğŸ“¦ KÃ­ch thÆ°á»›c TB: {sum(len(s.page_content) for s in splits) // len(splits)} kÃ½ tá»±/chunk")


def main():
    build_vectorstore()


if __name__ == "__main__":
    main()
